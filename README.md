# **Эталонное** решение HW 6

## 1. Описание

Задачи:

- Пишем скрипт `model_to_onnx.py` для конвертации чекпоинта из MLFlow в onnx формат (плюс сразу доп данные тоже подготавливаем).
- Реализуем api для сервиса-модели (MaaS) с помощью Nvidia Triton, создаем `model_repository` со структурой:
  - text_preprocess для предобработки данных (текстов). Тут мы используем python backend.
  - logreg для примения непосредственно модели, используем onnxruntime backend.
  - logreg_postprocess используем для выделения нужной нам информации, форматирования.
  - bot_classifier объединяет все шаги в одну ensemble модель.
- Изменяем логику нашего основного приложения, чтобы оно ходило в инференс сервис за предиктами через triton client.
- Добавляем кэш в redis-е, чтобы быстро отвечать на одинаковые запросы. 
- Пишем проверочные запросы в `tests/inference_requests.txt`, проверяем работоспособность.

## 2. Как запустить

1. Склонируйте репозиторий.
2. Убедитесь, что у вас установлены Docker и Docker Compose.
3. Настройте `.env`, пропишите все переменные окружения, которые будут использоваться в `docker-compose.yaml` и прочих файлах.
4. Запустите:
   ```bash
   docker-compose up -d --build
   ```
   Убедитесь, что контейнеры `postgres`, `minio`, `mlflow`, `redis`, `inference_service`, `youarebot` запустились без ошибок.
5. По-умолчанию модель запустится с моим заглушечным чекпоинтом, который лежит в репозитории. Для сборки своего поменяйте при необходимости названия, пути до модели в MLFlow, запустите:
   ```bash
   uv run model_to_onnx.py
   ```
   Этот скрипт расчитан для модели логистической регрессии с препроцессингом из ДЗ-4.

   Перезапустите сервисы после пересборки чекпоинта, повторив п.4.
6. Проверка сервиса:
   ```bash
   bash tests/inference_requests.txt
   ```
   Вы получите несколько json-ов в ответе, содержимое которых будет похоже на `tests/inference_responses.txt` (зависит от полученных чекпионтов на предыдущих этапах).

#### Примечание:
    Обычно данные и `.env` не пушатся в Git. Однако в данном случае это сделано для наглядности.
